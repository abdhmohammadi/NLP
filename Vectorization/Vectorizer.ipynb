{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "23b089cb",
      "metadata": {
        "id": "23b089cb"
      },
      "source": [
        "# Text Vectorization Example\n",
        "\n",
        "This notebook demonstrates how to build a simple text vectorizer from scratch. It covers:\n",
        "- Creating a vocabulary\n",
        "- Encoding text to numerical tokens\n",
        "- Decoding tokens back to text\n",
        "- Handling unknown (out-of-vocabulary) words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d795842",
      "metadata": {
        "id": "4d795842"
      },
      "source": [
        "## 1. Importing the Vectorizer\n",
        "\n",
        "We start by importing our custom `TextVectorization` module and initializing the `Vectorizer` class. Then, we prepare a small dataset to work with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4nHz_ZEnKizP",
      "metadata": {
        "id": "4nHz_ZEnKizP"
      },
      "source": [
        "First must be initalize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "vFmlnZH0KAZw",
      "metadata": {
        "id": "vFmlnZH0KAZw"
      },
      "outputs": [],
      "source": [
        "# Sample dataset of short sentences\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "93211287",
      "metadata": {
        "id": "93211287"
      },
      "outputs": [],
      "source": [
        "# Import the vectorizer module\n",
        "import TextVectorization as tv\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = tv.Vectorizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9929069b",
      "metadata": {
        "id": "9929069b"
      },
      "source": [
        "## 2. Creating Vocabulary\n",
        "\n",
        "Encoding/Decoding a Sentence\n",
        "\n",
        "We build the vocabulary from the dataset, encode a sentence into tokens (numbers), and then decode it back into text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "681a2c03",
      "metadata": {
        "id": "681a2c03",
        "outputId": "84526e39-fa73-41bd-96ab-475549455b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: [2, 3, 5, 7, 1, 5, 6]\n",
            "Decoded: i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary from the dataset\n",
        "vectorizer.make_vocabulary(dataset)\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "# Encode the sentence to tokens\n",
        "encoded = vectorizer.encode(test_sentence)\n",
        "print(f'Encoded: {encoded}')  # Example: [2, 3, 4, 5]\n",
        "\n",
        "# Decode the tokens back to words\n",
        "print(f'Decoded: {vectorizer.decode(encoded)}')  # Example: i write erase rewrite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "463fed94",
      "metadata": {
        "id": "463fed94"
      },
      "source": [
        "## 3. Handling Unknown Words\n",
        "\n",
        "Now we try encoding a sentence with words not in the original vocabulary. The unknown words are replaced by a special `[UNK]` token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "51ea9904",
      "metadata": {
        "id": "51ea9904",
        "outputId": "0a48e694-ec53-496a-a347-f4b18de2dfb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: [2, 1, 9, 1, 1]\n",
            "Decoded: i [UNK] a [UNK] [UNK]\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"I am a book lover\"\n",
        "encoded = vectorizer.encode(sample_text)\n",
        "print(f'Encoded: {encoded}') \n",
        "print(f'Decoded: {vectorizer.decode(encoded)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9ccd29",
      "metadata": {},
      "source": [
        "The result shows three `[KNK]` cases associated with three locations marked with `index = 1`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k4R3QM-nHvP7",
      "metadata": {
        "id": "k4R3QM-nHvP7"
      },
      "source": [
        "## 4. TextVectorization by Keras\n",
        "\n",
        "To build vocabulary in Kras, just call `tv.adapt(dataset)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2922b817",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from tensorflow) (78.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Using cached grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting h5py>=3.11.0 (from tensorflow)\n",
            "  Using cached h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Using cached ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Collecting namex (from keras>=3.5.0->tensorflow)\n",
            "  Using cached namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras>=3.5.0->tensorflow)\n",
            "  Using cached optree-0.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /mnt/829E4D099E4CF6E7/Projects/Python/nlp_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m518.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:29\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m635.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m523.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
            "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (404 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, markdown, grpcio, google-pasta, gast, absl-py, tensorboard, ml-dtypes, h5py, astunparse, keras, tensorflow\n",
            "\u001b[2K  Attempting uninstall: numpy[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/22\u001b[0m [opt-einsum]-data-server]\n",
            "\u001b[2K    Found existing installation: numpy 2.2.4━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/22\u001b[0m [opt-einsum]\n",
            "\u001b[2K    Uninstalling numpy-2.2.4:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/22\u001b[0m [numpy]]\n",
            "\u001b[2K      Successfully uninstalled numpy-2.2.4━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/22\u001b[0m [numpy]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [tensorflow]2\u001b[0m [tensorflow]]]\n",
            "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.2.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 keras-3.9.2 libclang-18.1.1 markdown-3.8 ml-dtypes-0.5.1 namex-0.0.9 numpy-2.1.3 opt-einsum-3.4.0 optree-0.15.0 protobuf-5.29.4 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "pWxsSyP1H3HW",
      "metadata": {
        "id": "pWxsSyP1H3HW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-14 15:27:09.755772: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-14 15:27:11.685760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747223832.299840    9923 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747223832.444498    9923 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1747223833.805279    9923 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747223833.805309    9923 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747223833.805310    9923 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747223833.805311    9923 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-14 15:27:13.986226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-14 15:27:48.838428: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m tv = TextVectorization(output_mode=\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# make a dictionary\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tv.adapt(\u001b[43mdataset\u001b[49m)\n",
            "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Create a text vectorizer based on keras layers\n",
        "tv = TextVectorization(output_mode=\"int\")\n",
        "\n",
        "# make a dictionary\n",
        "tv.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "988bd684",
      "metadata": {},
      "source": [
        "**Encoding/Decoding using `keras`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "AFAKQgQtPMKk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFAKQgQtPMKk",
        "outputId": "8f4bf45a-d08b-492e-e356-9e8eb3b2617e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded: [ 7  3  5  9  1  5 10]\n",
            "Decoded: i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "voc = tv.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded = tv(test_sentence)\n",
        "print(f'Encoded: {encoded}')\n",
        "\n",
        "inverse_voc = dict(enumerate(voc))\n",
        "\n",
        "decoded = \" \".join(inverse_voc[int(i)] for i in encoded)\n",
        "\n",
        "print(f'Decoded: {decoded}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff9e87b",
      "metadata": {},
      "source": [
        "the vector generated by `keras`  : [ 7, 3, 5, 9, 1, 5, 10]\n",
        "\n",
        "the vector generated from scratch: [2, 3, 5, 7, 1, 5, 6]\n",
        "\n",
        "the results is deferent becuase indexing method is deferent."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
