<style>
    body {
      font-family: 'Arial', sans-serif;
      line-height: 1.6;
      color: #333;
      background-color: #f4f4f4;
      margin: 20px;
    }
  
    div {
      background-color: #fff;
      padding: 30px;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
    }
  
    h2 {
      color: #007bff;
      border-bottom: 2px solid #007bff;
      padding-bottom: 10px;
      margin-bottom: 20px;
    }
  
    h3 {
      color: #28a745;
      margin-top: 20px;
      margin-bottom: 10px;
    }
  
    p {
      margin-bottom: 15px;
    }
  
    i {
      color: #6c757d;
    }
  
    a {
      color: #dc3545;
      text-decoration: none;
      font-weight: bold;
    }
  
    a:hover {
      text-decoration: underline;
    }
  </style>
<div>
    <h2>Text vectorization history</h2>
    <p>The concept of text vectorization in Natural Language Processing (NLP) has evolved over several decades, with contributions from various researchers and institutions.</p>
    <h3>Early Foundations</h3>
    <p>The foundational idea that "a word is characterized by the company it keeps" was proposed by British linguist John Rupert Firth in 1957. This principle, known as the <i>distributional hypothesis</i>, suggests that words used in similar contexts tend to have similar meanings. It laid the groundwork for representing words numerically based on their contextual usage.(<a href="https://en.wikipedia.org/wiki/Word_embedding">Wikipedia</a>, <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Wikipedia</a>)</p>
    <h3>Latent Semantic Analysis (LSA)</h3>
    <p>In the late 1980s, researchers Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum, and Lynn Streeter developed Latent Semantic Analysis (LSA). LSA is a technique that analyzes relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. It uses singular value decomposition (SVD) to reduce the dimensionality of the term-document matrix, capturing the underlying semantic structure in the data. (<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Wikipedia</a>)</p>
    <h3>Neural Probabilistic Language Models</h3>
    <p>In 2003, Yoshua Bengio and colleagues introduced the Neural Probabilistic Language Model, which aimed to address the curse of dimensionality in language modeling. This model proposed learning a distributed representation for words, allowing for better generalization to unseen word sequences. It marked a significant shift towards using neural networks for learning word representations.(<a href="https://en.wikipedia.org/wiki/Word_embedding">Wikipedia</a>)</p>
    <h3>Word2Vec</h3>
    <p>A major advancement came in 2013 when Tomáš Mikolov and his team at Google developed Word2Vec, a set of models that produce word embeddings. Word2Vec introduced two architectures: Continuous Bag of Words (CBOW) and Skip-Gram, both of which efficiently learn word representations that capture semantic and syntactic relationships. This work significantly popularized the use of dense vector representations in NLP.<a href="https://en.wikipedia.org/wiki/GloVe">Wikipedia</a></p>
    <h3>GloVe</h3>
    <p>In 2014, researchers at Stanford University, including Jeffrey Pennington, Richard Socher, and Christopher Manning, developed GloVe (Global Vectors for Word Representation). GloVe combines the benefits of global matrix factorization and local context window methods, producing word embeddings by aggregating global word-word co-occurrence statistics from a corpus.<a href="https://en.wikipedia.org/wiki/GloVe">Wikipedia</a></p>

    <h3>Summary</h3>
    While the idea of representing text numerically has roots in the mid-20th century, significant milestones include:

<li><b>1957</b>: Distributional hypothesis by John Rupert Firth.</li>
<li><b>Late 1980s</b>: Development of LSA by Deerwester et al.</li>
<li><b>2003</b>: Neural Probabilistic Language Model by Bengio et al.</li>
<li><b>2013</b>: Introduction of Word2Vec by Mikolov et al.</li>
<li><b>2014</b>: Development of GloVe by Pennington et al.(<a href="https://en.wikipedia.org/wiki/Word_embedding?utm_source=chatgpt.com">Wikipedia</a>, <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Wikipedia</a>, <a href="https://en.wikipedia.org/wiki/Word2vec">Wikipedia</a>)

</div>
