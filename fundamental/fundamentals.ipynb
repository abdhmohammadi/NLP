{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2c8b03",
   "metadata": {},
   "source": [
    "## <b>Summary of the Notebook</b>\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive exploration of fundamental Natural Language Processing (NLP) techniques.\n",
    "\n",
    "It includes practical implementations using popular libraries and frameworks such as Hugging Face Transformers, spaCy, \n",
    "\n",
    "and NLTK. The notebook is structured into multiple sections, each focusing on a specific NLP concept:\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Introduction to tokenization and its significance in NLP tasks. \n",
    "   - Demonstrated tokenization using Hugging Face's BERT tokenizer.\n",
    "   - Explanation of subword tokenization and handling out-of-vocabulary (OOV) words.\n",
    "\n",
    "2. **Lemmatization**:\n",
    "   - Definition and examples of lemmatization.\n",
    "   - Implementation using spaCy to convert words to their base or dictionary forms.\n",
    "   - Analysis of challenges and potential errors in lemmatization.\n",
    "\n",
    "3. **Stemming**:\n",
    "   - Explanation of stemming and its differences compared to lemmatization.\n",
    "   - Demonstration of stemming using NLTK's PorterStemmer.\n",
    "   - Side-by-side comparison of stemming and lemmatization results.\n",
    "\n",
    "4. **Stopwords Removal**:\n",
    "   - Importance of stopword removal in text preprocessing.\n",
    "   - Example implementation using NLTK's stopwords list.\n",
    "\n",
    "5. **Named Entity Recognition (NER)**:\n",
    "   - Introduction to NER and its applications.\n",
    "   - Examples of NER using spaCy and Hugging Face Transformers.\n",
    "   - Exploration of advanced NER features such as nested and overlapping entities.\n",
    "\n",
    "6. **References**:\n",
    "   - Links to foundational research papers and key resources for further reading.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3f3a0",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "1. Tokenization Using Hugging Face's Transformers (BERT Tokenizer)\n",
    "\n",
    "<b>defination</b>: Tokenization is the process of breaking down a piece of text into smaller units called tokens. Tokens can be words, \n",
    "\n",
    "sentences, or even characters, depending on the application.\n",
    "\n",
    "Hugging Face’s transformers library provides tokenizers specifically designed for transformer models like BERT, GPT, and others. \n",
    "\n",
    "Tokenization here often involves splitting the text into subwords rather than just words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c685173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pretrained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Hugging Face makes NLP easy!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b54f8f",
   "metadata": {},
   "source": [
    "***NOTICE:***\n",
    "\n",
    "The ## means \"This is a subword that continues from the previous token.\"\n",
    "So \"nl\" is the start of the word AND \"##p\" means it attaches to \"nl\" to form \"nlp\".\n",
    "\n",
    "\n",
    "Why doesn't BERT know some words as one token?\n",
    "\n",
    "The short answer is: because BERT has a fixed-size vocabulary, and that vocabulary doesn't include every possible word or acronym — only the most common ones seen during pretraining.\n",
    "\n",
    "Here’s why:\n",
    "\n",
    "1. <b>BERT is pretrained with a limited vocabulary</b>\n",
    "\n",
    "    BERT was trained on large datasets like BooksCorpus and English Wikipedia.\n",
    "\n",
    "    To keep the model size manageable, it uses a vocabulary of about 30,000 tokens.\n",
    "\n",
    "    These tokens are selected using a process (like WordPiece) that tries to balance:\n",
    "\n",
    "        Common full words (like \"science\", \"computer\"),\n",
    "\n",
    "        Frequent subwords (like \"##ing\", \"##tion\", etc.)\n",
    "\n",
    "Words that are too rare or appear in too many variations may not be stored as full tokens.\n",
    "\n",
    "2. <b>It’s more efficient to break rare or unseen words</b>\n",
    "\n",
    "    If BERT tried to store every possible word (including \"NLP\", \"OpenAI\", \"PySide\"), the vocabulary would be massive and impractical.\n",
    "\n",
    "    So instead, it learns to split rare/unseen words into known subwords.\n",
    "\n",
    "    This helps BERT generalize to words it never saw in training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb91fba2",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "\n",
    "* Sproat, R., & Shih, C. (1996). \"A Statistical Method for Tokenization.\" *Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL)*, 1996.\n",
    "\n",
    "  This paper discusses statistical methods for tokenization and how tokenization is essential for preprocessing in NLP tasks.\n",
    "\n",
    "**Link:** [ACL 1996 Paper on Tokenization](https://aclanthology.org/P96-1010/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baaf7e1",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "<b>Definition</b>: Lemmatization is the process of converting a word to its base or dictionary form (lemma), taking into account the context and grammar.\n",
    "\n",
    "Example:\n",
    "\n",
    "Words: \"am\", \"are\", \"is\" → Lemma: \"be\"\n",
    "\n",
    "Words: \"running\", \"ran\" → Lemma: \"run\"\n",
    "\n",
    "Lemmatization uses vocabulary and <b>morphological</b> analysis to return real words.\n",
    "\n",
    "Lemmatization with spaCy (Modern NLP Pipeline)\n",
    "\n",
    "spaCy is one of the most popular NLP libraries, offering high-performance lemmatization along with part-of-speech tagging and dependency parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26f36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'be', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pretrained spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence\n",
    "doc = nlp(\"The quick brown foxes are jumping over the lazy dogs.\")\n",
    "\n",
    "# Lemmatization\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd28016",
   "metadata": {},
   "source": [
    "In **Natural Language Processing (NLP)**, **morphology** refers to the study of the internal structure and \n",
    "\n",
    "formation of words. It involves analyzing how words are built from smaller meaningful units called **morphemes**, \n",
    "\n",
    "which are the smallest grammatical units in a language.\n",
    "\n",
    "### Key Concepts in Morphology:\n",
    "1. **Morphemes**:\n",
    "   - The smallest meaningful units in a language.\n",
    "   - Examples:  \n",
    "     - \"Unbreakable\" → \"un-\" (prefix, negates meaning) + \"break\" (root) + \"-able\" (suffix, means \"capable of\").\n",
    "     - \"Cats\" → \"cat\" (root) + \"-s\" (plural suffix).\n",
    "\n",
    "2. **Types of Morphemes**:\n",
    "   - **Free Morphemes**: Can stand alone as words (e.g., \"book\", \"run\").\n",
    "   - **Bound Morphemes**: Must attach to other morphemes (e.g., \"-ing\", \"un-\").\n",
    "\n",
    "3. **Morphological Processes**:\n",
    "   - **Inflection**: Modifies a word to fit grammatical rules without changing its core meaning (e.g., \"run\" → \"runs\", \"running\").\n",
    "   - **Derivation**: Creates new words with new meanings (e.g., \"happy\" → \"unhappy\", \"quick\" → \"quickly\").\n",
    "   - **Compounding**: Combining words to form new ones (e.g., \"blackboard\", \"sunflower\").\n",
    "   - **Cliticization**: Attaching reduced forms of words (e.g., \"I'm\" → \"I\" + \"am\").\n",
    "\n",
    "4. **Morphological Analysis in NLP**:\n",
    "   - **Tokenization**: Splitting text into words or subwords.\n",
    "   - **Stemming**: Reducing words to their base form (e.g., \"running\" → \"run\").\n",
    "   - **Lemmatization**: Converting words to their dictionary form (e.g., \"better\" → \"good\").\n",
    "   - **Morphological Segmentation**: Breaking words into morphemes (useful for agglutinative languages like Turkish or Finnish).\n",
    "\n",
    "### Importance in NLP:\n",
    "- Helps in **text normalization**, **machine translation**, **spell checking**, and **information retrieval**.\n",
    "- Improves handling of **rare or unseen words** by breaking them into morphemes.\n",
    "- Essential for languages with **rich morphology** (e.g., Arabic, Persian, German, Turkish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd233c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the' 'goose' \"'s\" 'foot' 'be' 'hurt']\n",
      "['as' 'they' 'run' 'quickly' 'through' 'the']\n",
      "['leave' ',' 'avoid' 'fall' 'branch' '.']\n",
      "[\"'\" 'she' 'think' 'the' 'mouse' 'have']\n",
      "['hide' 'well' 'in' 'previous' 'search' ',']\n",
      "['but' 'her' 'analysis' 'show' 'otherwise' '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"The geese's feet were hurting as they ran quickly through the leaves, avoiding fallen branches. '\" \\\n",
    "\"She thought the mice had hidden better in previous searches, but her analysis showed otherwise.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "import numpy as np\n",
    "\n",
    "lemmas = np.reshape(lemmas,(6, int(len(lemmas)/6)))\n",
    "for lem in lemmas:\n",
    "    print(lem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4a23a",
   "metadata": {},
   "source": [
    "***Analysis:***\n",
    "\n",
    "'leave'    ❌\n",
    "\n",
    "    \"leaves\" → \"leave\": Incorrect!\n",
    "\n",
    "        Expected: \"leaf\" (noun, \"tree leaves\").\n",
    "\n",
    "        Why? spaCy likely misinterpreted \"leaves\" as the verb (\"she leaves the room\"). Context matters!\n",
    "\n",
    "'well'    ❓\n",
    "\n",
    "    \"better\" → \"well\" (technically correct, but contextually \"good\" might be expected).\n",
    "\n",
    "        \"Better\" is the comparative of both \"good\" and \"well\". spaCy defaults to \"well\".\n",
    "\n",
    "***Key Observations:***\n",
    "\n",
    "    Ambiguity Errors:\n",
    "\n",
    "        \"leaves\" → \"leave\" (incorrect due to noun/verb ambiguity).\n",
    "\n",
    "            Fix: Use POS tagging to force noun interpretation (e.g., token.lemma_ if token.pos_ != \"VERB\" else \"leaf\").\n",
    "\n",
    "    Comparative Adjectives:\n",
    "\n",
    "        \"better\" → \"well\" (not \"good\"):\n",
    "\n",
    "            This is technically correct but may not fit the semantic intent (\"good\" is the base form for \"better\" in many contexts).\n",
    "\n",
    "    Possessive ('s):\n",
    "\n",
    "        Tokenized separately (\"goose\" + \"'s\"), which is standard in NLP pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd54cef",
   "metadata": {},
   "source": [
    "***How to Improve Accuracy:***\n",
    "\n",
    "    Use POS Tags\n",
    "\n",
    "`lemmas = [token.lemma_ if token.pos_ != \"VERB\" else \"leaf\" for token in doc]`\n",
    "\n",
    "(Manually override for known errors.)\n",
    "\n",
    "***Custom Rules:***\n",
    "Add exceptions for words like \"better\" if you prefer \"good\":\n",
    "python\n",
    "\n",
    "`lemma = \"good\" if token.text == \"better\" else token.lemma_` \n",
    "\n",
    "\n",
    "***<p style=\"color:red\"> But both above methods cannot be automated in general.</b>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce500a6",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "\n",
    "* Finkel, J. R., Grenager, T., & Manning, C. D. (2005). \"Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.\" *Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)*, 2005.\n",
    "\n",
    "  This paper discusses the concept of lemmatization and how it can be used in conjunction with other techniques for enhancing NLP tasks like named entity recognition.\n",
    "\n",
    "**Link:** [ACL 2005 Paper on Lemmatization](https://aclanthology.org/P05-1015/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcc489",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "<b>Definition:</b>\n",
    "Stemming is the process of reducing a word to its root form by cutting off prefixes or suffixes. \n",
    "\n",
    "Unlike lemmatization, stemming does not consider grammar or context and might produce non-real words.\n",
    "\n",
    "Example:\n",
    "Words: \"running\", \"runner\", \"runs\" → Stem: \"run\" or \"runn\"\n",
    "\n",
    "Stemmers often produce more aggressive cuts (e.g., \"studies\" → \"studi\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25df6af",
   "metadata": {},
   "source": [
    "\n",
    "<b>Using NLTK (Traditional Method)</b>\n",
    "\n",
    "For practicing traditional stemming methods, you can use NLTK's PorterStemmer. While it's less \n",
    "\n",
    "accurate than lemmatization, it's still widely used for some tasks where high precision isn’t as important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73741426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'ran', 'happili']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"ran\", \"happily\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6894f",
   "metadata": {},
   "source": [
    "***Stemming vs. lemmatization***\n",
    "\n",
    "Stemming and lemmatization are both text normalization techniques in NLP, but they operate differently.\n",
    "\n",
    "🔹 Stemming\n",
    "\n",
    "Reduces a word to its root form by chopping off suffixes.\n",
    "\n",
    "It may not produce real words.\n",
    "\n",
    "Typically faster and more aggressive.\n",
    "\n",
    "Example: running, runs → run\n",
    "\n",
    "🔹 Lemmatization\n",
    "\n",
    "Reduces a word to its dictionary base form (lemma) using vocabulary and grammar.\n",
    "\n",
    "Produces valid words.\n",
    "\n",
    "Requires POS (part-of-speech) tagging for accuracy.\n",
    "\n",
    "Example: better → good, running → run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764aaf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abdh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/abdh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/abdh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required data (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f795054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tStemmed\t\tLemmatized\n",
      "----------------------------------------\n",
      "running     run         running     \n",
      "flies       fli         fly         \n",
      "better      better      better      \n",
      "studies     studi       study       \n",
      "children    children    child       \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample words\n",
    "words = [\"running\", \"flies\", \"better\", \"studies\", \"children\"]\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Word\\t\\tStemmed\\t\\tLemmatized\")\n",
    "print(\"-\" * 40)\n",
    "for word in words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)  # Default is noun lemmatization\n",
    "    print(f\"{word:<12}{stemmed:<12}{lemmatized:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6424d47",
   "metadata": {},
   "source": [
    "***Note:***\n",
    "\n",
    "stemmer.stem() removes suffixes mechanically.\n",
    "\n",
    "lemmatizer.lemmatize() returns valid dictionary words,\n",
    "\n",
    "but you can improve accuracy by specifying POS (e.g., lemmatize(word, pos='v'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ef6235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "out_ = lemmatizer.lemmatize(\"running\", pos=\"v\")  # Output: run\n",
    "\n",
    "print(out_)\n",
    "\n",
    "out_ = lemmatizer.lemmatize(\"better\", pos=\"a\")   # Output: good (with WordNet enhancement)\n",
    "\n",
    "print(out_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ad410",
   "metadata": {},
   "source": [
    "\n",
    "In lemmatization, the `pos` parameter stands for **Part of Speech**, which tells the lemmatizer \n",
    "\n",
    "what **grammatical role** the word plays (e.g., verb, noun, adjective). This is important because \n",
    "\n",
    "the same word can have different lemmas depending on its role in a sentence.\n",
    "\n",
    "Here are common POS tags used in `nltk.WordNetLemmatizer`:\n",
    "\n",
    "| POS Tag | Meaning         | Example         | Lemmatized Form |\n",
    "|---------|------------------|------------------|------------------|\n",
    "| `'n'`   | Noun             | `cars` → `car`   | `car`            |\n",
    "| `'v'`   | Verb             | `running` → `run`| `run`            |\n",
    "| `'a'`   | Adjective        | `better` → `good`| `good`           |\n",
    "| `'r'`   | Adverb           | (less common)    | `quickly` → `quickly` |\n",
    "\n",
    "\n",
    "Without the correct POS, the lemmatizer **assumes the word is a noun** by default. That can lead to incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d97a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Default (noun)\n",
    "print(lemmatizer.lemmatize(\"running\"))           # ➜ running\n",
    "# With verb POS\n",
    "print(lemmatizer.lemmatize(\"running\", pos='v'))  # ➜ run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265cb63",
   "metadata": {},
   "source": [
    "> Notice: Without `pos='v'`, `\"running\"` doesn't get reduced to `\"run\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989c526",
   "metadata": {},
   "source": [
    "### Summary of POS Tags for Lemmatization\n",
    "\n",
    "| Symbol | Full Part of Speech | Used For            |\n",
    "|--------|---------------------|----------------------|\n",
    "| `'n'`  | Noun                | `cat`, `house`, `idea` |\n",
    "| `'v'`  | Verb                | `run`, `is`, `studying` |\n",
    "| `'a'`  | Adjective           | `big`, `better`, `beautiful` |\n",
    "| `'r'`  | Adverb              | `quickly`, `silently` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b341fa",
   "metadata": {},
   "source": [
    "`More practice:` Evaluating how stemming might affect the accuracy of downstream tasks like classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420661e",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "\n",
    "* Porter, M. F. (1980). \"An Algorithm for Suffix Stripping.\" *Program*, 14(3), 130-137.\n",
    "\n",
    "  This is the foundational paper for the **Porter Stemmer**, one of the most widely used \n",
    "  \n",
    "  stemming algorithms. It explains how stemming works by stripping off word suffixes.\n",
    "\n",
    "**Link:** [Porter 1980 Stemming Paper](https://www.aclweb.org/anthology/J80-2003/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977bef7f",
   "metadata": {},
   "source": [
    "## Stopwords:\n",
    "\n",
    "<b>Definition:</b> Stopwords are common words that are often removed from text during preprocessing \n",
    "\n",
    "because they carry little meaningful information for tasks like classification or search.\n",
    "\n",
    "Examples:\n",
    "\"the\", \"is\", \"in\", \"and\", \"to\", \"of\", \"a\", \"on\"\n",
    "\n",
    "These are filtered out to reduce `noise` and focus on meaningful content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a935272",
   "metadata": {},
   "source": [
    "\n",
    " ***Stopwords removal using nltk (Classic Approach)***\n",
    "\n",
    "Stopwords removal is a simple but effective preprocessing step. You can use NLTK's list of stopwords for English, or create custom lists based on your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78eaa09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'stopwords.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load NLTK's stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"This is an example sentence with stopwords.\"\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_sentence = [word for word in sentence.split() if word.lower() not in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb68886",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "\n",
    "* Luhn, H. P. (1957). \"The Automatic Creation of Literature Abstracts.\" *IBM Journal of Research and Development*, 1(2), 159-165.\n",
    "\n",
    "  Luhn discusses the role of stopwords in information retrieval systems and introduces methods for filtering out irrelevant words to focus on content-bearing terms.\n",
    "\n",
    "**Link:** [Luhn 1957 Paper on Stopwords](https://ieeexplore.ieee.org/document/5393933)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8b87a",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) allows us to detect entities like person names, organizations, dates, etc., \n",
    "\n",
    "in the text. spaCy provides a fast and efficient NER model.\n",
    "\n",
    "***Named Entity Recognition (NER) Using spaCy***\n",
    "\n",
    "***Example (1)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d957e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "UK GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load pretrained spaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence with entities\n",
    "doc = nlp(\"Apple is looking to buy a startup in the UK for $1 billion.\")\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f295024",
   "metadata": {},
   "source": [
    "***Example (2)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6331f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 25, 2024                  DATE        Absolute or relative dates or periods\n",
      "Sam Altman                      PERSON      People, including fictional\n",
      "the European Parliament         ORG         Companies, agencies, institutions, etc.\n",
      "Brussels                        GPE         Countries, cities, states\n",
      "Ursula von der                  PERSON      People, including fictional\n",
      "Leyen                           GPE         Countries, cities, states\n",
      "Elon Musk                       PERSON      People, including fictional\n",
      "Tesla                           ORG         Companies, agencies, institutions, etc.\n",
      "Palo Alto                       GPE         Countries, cities, states\n",
      "California                      GPE         Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's large English model (better accuracy)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Example: A sentence with nested and overlapping entities\n",
    "text = \"\"\"\n",
    "On April 25, 2024, OpenAI CEO Sam Altman visited the European Parliament in Brussels to discuss AI policy with Ursula von der Leyen.\n",
    "Later, he met with Elon Musk at the Tesla headquarters in Palo Alto, California.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities with labels and their explanations\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<30}  {ent.label_:<10}  {spacy.explain(ent.label_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ecfcb",
   "metadata": {},
   "source": [
    "This example shows several key features of advanced NER:\n",
    "\n",
    "Feature\tExplanation\n",
    "\n",
    "Multiple Entity Types\tspaCy correctly identifies various types like PERSON, ORG, GPE, and DATE.\n",
    "\n",
    "Nested Structure\tPhrases like \"European Parliament in Brussels\" contain a nested ORG and GPE, and spaCy extracts them independently.\n",
    "\n",
    "Ambiguity Handling\tWords like \"Tesla\" can refer to a person or a company. Contextually, spaCy identifies it as an ORG.\n",
    "\n",
    "Disambiguation\tIt knows \"Ursula von der Leyen\" is a PERSON and not confused by the length or title."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d86fe",
   "metadata": {},
   "source": [
    "***Reference:***\n",
    "\n",
    "\"spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing\"\n",
    "\n",
    "Authors: Matthew Honnibal and Ines Montani\n",
    "\n",
    "Conference: To appear in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n",
    "\n",
    "Abstract: This paper presents the architecture behind spaCy, including its NER system, which uses a transition-based parser and deep learning (CNNs) to accurately detect named entities.\n",
    "\n",
    "\n",
    "**Link:** [Read the Paper (via arXiv)](https://arxiv.org/abs/1907.08237)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f778381",
   "metadata": {},
   "source": [
    "***Example(3)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcafe1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 15th, 2023        | DATE       | Absolute or relative dates or periods\n",
      "OpenAI                    | ORG        | Companies, agencies, institutions, etc.\n",
      "Microsoft                 | ORG        | Companies, agencies, institutions, etc.\n",
      "San Francisco             | GPE        | Countries, cities, states\n",
      "$10 billion               | MONEY      | Monetary values, including unit\n",
      "AI                        | GPE        | Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Complex sentence with multiple entity types\n",
    "text = (\"On January 15th, 2023, OpenAI announced a strategic partnership \"\n",
    "        \"with Microsoft in San Francisco, investing $10 billion to accelerate \"\n",
    "        \"AI research and deployment worldwide.\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Display named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<25} | {ent.label_:<10} | {spacy.explain(ent.label_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04d957",
   "metadata": {},
   "source": [
    "***Analysis:***\n",
    "\n",
    "DATE: Recognizes the specific date when the event occurred.\n",
    "\n",
    "ORG: Detects organizations involved in the event, i.e., OpenAI and Microsoft.\n",
    "\n",
    "GPE (Geo-Political Entity): Correctly detects San Francisco as a city.\n",
    "\n",
    "MONEY: Detects a large investment amount: $10 billion.\n",
    "\n",
    "LOC: Identifies \"worldwide\" as a location, which is contextually less specific than a GPE.\n",
    "\n",
    "This example shows how NER models capture structured information (like date, money, orgs, locations) from unstructured text, which is crucial for tasks like:\n",
    "\n",
    "Information extraction\n",
    "\n",
    "Knowledge base population\n",
    "\n",
    "Event detection\n",
    "\n",
    "Question answering\n",
    "\n",
    "***Reference***\n",
    "\n",
    "The underlying method spaCy uses is inspired by statistical and neural models for sequence labeling. A foundational and widely cited scientific paper is:\n",
    "🔹 Lample et al., 2016\n",
    "\n",
    "\"Neural Architectures for Named Entity Recognition\"\n",
    "\n",
    "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer\n",
    "\n",
    "Proceedings of NAACL 2016\n",
    "\n",
    "Abstract Summary: This paper proposes a deep learning model using Bidirectional LSTMs and CRF (BiLSTM-CRF) \n",
    "\n",
    "for NER, achieving state-of-the-art performance without hand-crafted features.\n",
    "\n",
    "**Links:**\n",
    " \n",
    "[ACL Anthology](https://aclanthology.org/N16-1030/)\n",
    "\n",
    "[ arXiv preprint](https://arxiv.org/abs/1603.01360)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf3e04",
   "metadata": {},
   "source": [
    "***Named Entity Recognition (NER) Using BERT***\n",
    "\n",
    "***Example (4)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line if needed\n",
    "#%pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b501e1",
   "metadata": {},
   "source": [
    "***Load NER model(Online)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87175005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load transformer-based NER model\n",
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e53726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI               ORG        Score: 1.00\n",
      "Microsoft            ORG        Score: 1.00\n",
      "San Francisco        LOC        Score: 1.00\n",
      "AI                   MISC       Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Test sentence\n",
    "text = (\"On January 15th, 2023, OpenAI announced a strategic partnership \"\n",
    "        \"with Microsoft in San Francisco, investing $10 billion to accelerate \"\n",
    "        \"AI research and deployment worldwide.\")\n",
    "\n",
    "# Run NER\n",
    "results = ner(text)\n",
    "\n",
    "# Print results\n",
    "for entity in results:\n",
    "    print(f\"{entity['word']:20} {entity['entity_group']:10} Score: {entity['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr                   PER        Score: 0.84\n",
      "Worldwide            PER        Score: 0.44\n",
      "Miami                LOC        Score: 1.00\n",
      "Lisbon               LOC        Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Test sentence\n",
    "text = \"Mr. Worldwide performed in Miami and Lisbon during his tour.\"\n",
    "\n",
    "# Run NER\n",
    "results = ner(text)\n",
    "\n",
    "# Print results\n",
    "for entity in results:\n",
    "    print(f\"{entity['word']:20} {entity['entity_group']:10} Score: {entity['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644c2e4",
   "metadata": {},
   "source": [
    "## Text Classification Using Pretrained BERT (Fine-tuning)\n",
    "\n",
    "You can fine-tune a pretrained BERT model on a text classification task. This involves training \n",
    "\n",
    "the model on a labeled dataset for a specific task, such as sentiment analysis or topic categorization.\n",
    "Code Sample (Fine-tuning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f19aa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/abdh/Other/Projects/Python/abdh_env/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install accelerate>=0.26.0\n",
    "%pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094db4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d86a46f19c34256afae14886f333e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   2%|1         | 10.5M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/gpt2/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c32af5f7194700bcd8b9e26fcc932b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  17%|#7        | 94.4M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5b711548d14712bc76d9966b4f8bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c815b54c4c54404b5f300557fbd4979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d549ffb57f74c3096ee8d3042831f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d35d9e046ca4435aa95efce68d08ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aa367d43ff4fbea8f90fe64a152164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63adb7eea4fd4c0188e8b60574b00981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5481a523506d4165a5ee80a859e86239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dedaac3f0c4e029959d6105dcd4453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e8c961127a41bb97644f6f7bb2816f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6931be25614f50b598ed0bb819bda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4683fa6be2854dbaa6fc56d15baf8f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb137f5c401440faa050d9da7128f3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbe9433765a432fb342f8a36511ead9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m tokenized_dataset = dataset.map(tokenize_function, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./gpt2-finetuned\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use mixed precision if possible\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n\u001b[32m     40\u001b[39m trainer = Trainer(\n\u001b[32m     41\u001b[39m     model=model,\n\u001b[32m     42\u001b[39m     args=training_args,\n\u001b[32m     43\u001b[39m     train_dataset=tokenized_dataset,\n\u001b[32m     44\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     45\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPT-2 doesn't have a pad token; use eos_token as a workaround\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load a small dataset (replace with your dataset)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split='train[:1%]')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    # We concatenate the text and use the same as label for causal LM\n",
    "    tokens = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=torch.cuda.is_available(),  # use mixed precision if possible\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92c8ce",
   "metadata": {},
   "source": [
    "## Question Answering(QA)\n",
    "\n",
    "Using Hugging Face's Pretrained Models\n",
    "\n",
    "Another exciting task to explore is question answering using pretrained models like BERT or T5.\n",
    "\n",
    "You can use models trained on QA tasks like SQuAD to answer questions based on given context.\n",
    "Code Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21341007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8413013219833374, 'start': 292, 'end': 320, 'answer': 'improve educational outcomes'}\n",
      "{'score': 0.29887500405311584, 'start': 7, 'end': 42, 'answer': 'data science graduate, math teacher'}\n",
      "{'score': 0.2886771857738495, 'start': 48, 'end': 58, 'answer': 'book lover'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pretrained question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Example context and question\n",
    "context = \"I am a data science graduate, math teacher, and book lover. \" \\\n",
    "\"I am passionate about combining advanced analytics with educational excellence to create innovative learning experiences. \" \\\n",
    "\"With expertise in both fields, I strive to make complex concepts accessible and engaging for students, \" \\\n",
    "\"and to improve educational outcomes using data-driven approaches.\"\n",
    "question1 = \"What is my porpose using datascience approaches?\"\n",
    "\n",
    "# Get answer\n",
    "result = qa_pipeline(question=question1, context=context)\n",
    "print(result)\n",
    "\n",
    "question2 = \"What is my job?\"\n",
    "\n",
    "# Get answer\n",
    "result = qa_pipeline(question=question2 ,context=context)\n",
    "print(result)\n",
    "\n",
    "question1 = \"Which is my main job? data science or math teacher\"\n",
    "\n",
    "# Get answer\n",
    "result = qa_pipeline(question=question1, context=context)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240846d",
   "metadata": {},
   "source": [
    "**Analysis** \n",
    "\n",
    "<b>Code Summary</b>\n",
    "\n",
    "This loads a **pretrained QA model**, typically `distilbert-base-cased-distilled-squad`, \n",
    "\n",
    "trained on the **SQuAD dataset** (Stanford Question Answering Dataset). It can extract \n",
    "\n",
    "answers from a given context based on a natural-language question.\n",
    "\n",
    "\n",
    "<b>Input Context</b>\n",
    "\n",
    "You describe yourself as:\n",
    "\n",
    "* A **data science graduate**, **math teacher**, and **book lover**.\n",
    "* Passionate about combining **analytics** and **education**.\n",
    "* Aiming to make **learning more engaging** and **outcome-focused** using **data**.\n",
    "\n",
    "\n",
    "\n",
    "***Output Analysis***\n",
    "\n",
    "🔹 Question 1:\n",
    "\n",
    "**\"What is my purpose using datascience approaches?\"**\n",
    "\n",
    "```json\n",
    "{'score': 0.841, 'answer': 'improve educational outcomes'}\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "* The model correctly identifies our **goal**: using data science to **improve educational outcomes**.\n",
    "* High confidence (`score: 0.84`) indicates strong match.\n",
    "\n",
    "✅ **Good answer**, aligned with our intent.\n",
    "\n",
    "🔹 Question 2:\n",
    "\n",
    "**\"What is my job?\"**\n",
    "\n",
    "```json\n",
    "{'score': 0.298, 'answer': 'data science graduate, math teacher'}\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "* It correctly picks up **professions/roles** from the context.\n",
    "* The score is **low (0.29)** → the model is **uncertain**, probably because:\n",
    "\n",
    "  * The context lists multiple roles (graduate, teacher, book lover).\n",
    "  * \"Job\" is ambiguous: Are you asking for **current** job, **main** job, or **background**?\n",
    "\n",
    "**Acceptable answer**, but not confident.\n",
    "\n",
    "🔹 Question 3:\n",
    "\n",
    "**\"Which is my main job? data science or math teacher\"**\n",
    "\n",
    "```json\n",
    "{'score': 0.288, 'answer': 'book lover'}\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "* This is a **multi-option question**, requiring **comparison/reasoning**, which the QA model isn’t trained to handle well.\n",
    "* It extracts **'book lover'**, which isn't even a valid choice.\n",
    "* The **low score (0.28)** shows the model is **guessing**.\n",
    "\n",
    "**Incorrect answer** due to:\n",
    "\n",
    "* Question being outside the model’s capability (it doesn't reason between alternatives well).\n",
    "* No clear signal in context for \"main\" job.\n",
    "\n",
    "---\n",
    "\n",
    "***Overall Observations***\n",
    "\n",
    "| Question                    | Answer                                | Score | Quality |\n",
    "| --------------------------- | ------------------------------------- | ----- | ------- |\n",
    "| Purpose of data science use | `improve educational outcomes`        | 0.84  | ✅ Good  |\n",
    "| What is my job?             | `data science graduate, math teacher` | 0.29  | 🟡 Okay |\n",
    "| Which is my main job?       | `book lover`                          | 0.28  | 🔴 Poor |\n",
    "\n",
    "---\n",
    "\n",
    "*Insights*\n",
    "\n",
    "1. **QA models extract text spans**, they don’t *infer* or *reason* well.\n",
    "2. Questions like “which is better?” or “what is the main…” often need **reasoning**, which basic models like `distilbert` can’t handle.\n",
    "3. Context clarity matters. More explicit job role info would help.\n",
    "\n",
    "---\n",
    "\n",
    "***Reference***\n",
    "\n",
    "The model is based on:\n",
    "\n",
    "> **DistilBERT**, trained on **SQuAD v1.1**\n",
    "> (Rajpurkar et al., 2016 – *SQuAD: 100,000+ Questions for Machine Comprehension of Text*)\n",
    "> [https://aclanthology.org/D16-1264/](https://aclanthology.org/D16-1264/)\n",
    "\n",
    "---\n",
    "\n",
    "***Recommendation***\n",
    "\n",
    "For better performance:\n",
    "\n",
    "* Use a more advanced model like `deepset/roberta-base-squad2` or `bert-large-uncased-whole-word-masking-finetuned-squad`.\n",
    "* Structure your context more explicitly (e.g., “My primary job is…”).\n",
    "* Consider using an **LLM QA system** (like OpenAI’s `gpt-3.5` or `gpt-4`) if reasoning is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da83944",
   "metadata": {},
   "source": [
    "## Using GPT for Text Generation\n",
    "\n",
    "GPT models can generate coherent and contextually relevant text based on a given prompt. \n",
    "\n",
    "You can experiment with GPT-3-like models (from OpenAI) for tasks like text completion, summarization, and creative writing.\n",
    "\n",
    "Code Sample (Text Generation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b93986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pretrained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode prompt\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode and print output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (abdh_env)",
   "language": "python",
   "name": "abdh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
